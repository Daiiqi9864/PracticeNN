# PracticeNN
**神经网络简介**

如果把人工智能按照要解决的问题，分为监督学习、非监督学习、强化学习（靠过程验证），那么可以做以下简单理解：模型特征的提取（仅有输入，无标签）属于非监督学习，模型特性的辨识（输入-输出拟合）属于监督学习，但模型表达的辨识（输入的正确与否只能在过程中评估）属于强化学习。

神经网络是人工智能采用的工具之一。神经网络（Neural Network，NN）即由神经元连接的，输入与输出间的一段传递关系。该传递关系结构已设定，参数可更新。输入与输出间的过程一般定义为层。神经网络可以有各种形状。在前向神经网络（FNN）中，一个线性隐藏层通常由线性传递关系与激活函数组成，线性传递关系可表达为

y=Wx+b

其中x为输入，y为输出，W为权重参数，b为偏置参数；激活函数通常为非线性单调增函数，用于向神经网络中引入非线性，使神经网络可以表达非线性特性。在卷积神经网络中，卷积层执行输入数据与核函数的卷积，并将输出进行池化滤波，以提取高维特征，其中核函数为一含参数组或函数。



神经网络听起来很厉害的地方就在于只要规模足够大，理论上就可以用上述的一般结构表达所有的非线性过程。但是因为搞电路建模的人通常都需要可靠的理论模型，所以一般不会需要用神经网络模拟未知的非线性过程，也不能容忍神经网络这种未解析的非线性过程。

但这里还是说一下，AI在电气领域能应用的地方。

  用AI优化模型-监督学习
  
  让AI替代模型-监督学习或强化学习
  
  用AI优化控制器-监督学习
  
  让AI成为控制器-监督学习或强化学习
  
如果你只需要整定参数，建议用最小二乘、粒子群、梯度下降（不要用机器学习，参数整定的决策不涉及状态转移，用了也白用）。

如果你需要辨识函数，仅在你不需要解析式的时候才建议用神经网络，在你需要解析式的时候还是建议用最小二乘。

如果要设计智能控制器，可以用神经网络来搭非线性传递过程，也可以用机器学习来搭决策机。

如果要智能识别解析的函数表达式，建议转到符号学习（机器学习）。

AI涉及的，改一改就能发文的点有：预处理与正则化、改图与对抗（电气专业不要轻易碰这个，拿来复现就好）、根据功能拼接图（可以声称这是根据实际问题的处理）、预训练。



听说，关于FNN之外的几种神经网络的结构：

LSTM提取时间序列的标量特征

seq2seq编码解码建立函数映射

transferlearning实现从预训练模型到新数据模型的迁移

transformer比LSTM更准

听说，如果开发了可能会比较有用的轮子：matlab仿真采样、打标签、数据筛选的python端自动化。



**例程简介**

使用神经网络的环境部署：安装CUDA驱动，安装cuDNN库，安装Anaconda并建立虚拟环境，通过Anaconda安装python，tensorflow-gpu或torch-（CUDA版本号），通过pip安装其余支持库，并保证所有工具与支持库之间版本兼容。

*旧例程很多是用tensorflow（tf）写的，但由于tf糟糕的版本管理与兼容情况，建议读通原理后手动迁移至pytorch实现。不建议直接编译tf例程。

具体例程代码就是example_pytorch和example_tensorflow。功能上一般只需要包括：

1. 样本数据（input, target）的读取（例程里用的是生成数据）

2. 用框架自带功能打包数据集，以便使用时随机取小样本batch（只在pytorch例程里写了）

3. 只是用框架自带的功能，定义并初始化神经网络。或者

   定义可训练参数 + 定义前向过程（输入经参数计算得输出的函数） + 定义loss计算方法 + 定义梯度下降方法。用这种方法的时候，可以不定义神经网络，而是定义一般的传递图，从而基于tf或pytorch中自带的loss计算方法与梯度下降方法，实现非神经网络的一般问题参数优化。

   也就是，完全植入物理模型的神经网络，其实就是梯度下降法参数优化。

   在这一步之后，如果之前保存了模型状态，可以读取以继续训练。

4. 训练。
   为了训练一个网络，需要进行多个epoch，每个epoch都对数据集做一次遍历。在一个epoch里，为了遍历数据集，要多次对数据集进行不重复的小型batch取样。对于每组batch：

   将其中的input输入网络；

   输出与target比较得到误差作为loss（取平均，或均方根，或L1, L2之类。也可以自己定义loss，保证loss为零的情况仅对应理想情况就行）；

   计算loss关于神经网络中所有可训练参数的偏导（梯度）；

   按梯度下降的原则（实际可以有各种优化算法）更新模型参数。

   通常，虽然每个batch都会训练一次神经网络，但包括loss在内的日志通常在epoch达到某个阶段值时打印。例如，总共训练5000个epoch，每100个epoch打印一次日志。

6. 用框架自带的断点功能保存模型状态。

7. 将自定义输入量输入神经网络（或前向过程），得到输出，验证结果。

在上述过程中，若需要在GPU上计算，则需将神经网络（或可训练参数）与batch上传到GPU再执行计算。



**PINN的应用场景**

假设已确定存在一对输入输出关系，输入为x，输出为u，以下均以标量表示，实际问题中可以为矢量。当该关系不能直接求解时，可认为它们之间存在隐关系u = u(x)。时间自变量t可认为是x的一个维度，也可作为独立变量，则u = u(x, t)。

在灰箱问题中，虽然无法对u(x, t)建立准确模型，但出于对模型的知识，预先知道它满足的一些约束。比如（以下符号均为方便理解，没有特殊规定）：

1）u关于x的含参偏微分方程，也可以简写成包含u的式f(x, t, λ) = 0；

2）u或包含u的式φ的边界条件或周期条件，以等式表达；

3）u的限制条件，以不等式表达。

由于1）2）都可以表示成右半为零的等式形式，3）也可以通过拉格朗日乘数法或变分不等式构造成等式，所以限制条件可以表达成泛函，例如

![image](https://github.com/Daiiqi9864/PracticeNN/assets/172522013/615af530-55d1-4cb0-aeb2-2d87775907ad)

物理信息神经网络（Physics-Informed Neural Network，PINN）能够解决如此描述的问题：已确定x与u间有输入输出关系，x可测；存在偏微分方程条件N1[u]已知，其他部分条件Ni[u]已知。

PINN由神经网络与物理模型两部分构成。由于神经网络可通过线性加权与（大部分情况单调的）非线性激励函数模拟非线性函数，并且其在训练中可通过梯度下降的方式调整权重w与偏置b，以及可能存在的其他参数；因此认为神经网络可以用于表达x到u的未知输入输出关系。对于神经网络部分输出up，基于物理模型N1[u]可搭建计算通道，模型中参数λ可视为与神经网络中参数同样的待优化对象；由于模型表达为右侧为零的等式形式，前向通道输出取绝对值或均方根可作为损失函数E，用于优化所有参数。

在根据实际问题构造PINN时，需注意：u不一定实值可观测，不一定是实际存在的物理量；Ni不一定有物理意义，其作用仅为模型约束；当N1[u]已知且为唯一的损失函数来源时，可通过链式法则直接计算E关于包括λ与各层w, b在内的，各参数的偏导，从而加速梯度下降计算。

PINN可解决的问题分为两类：

  数据驱动求解（Data-driven solutions）：λ已知，神经网络拟合u(x)，即对偏微分方程N1[u]计算数值解；

  数据驱动辨识（Data-driven discovery）：训练神经网络以辨识λ。

依托于基于微分方程f(x, t, λ) = 0 拟合非线性过程u(x)与参数λ的特点，PINN在电气工程领域的应用以数据驱动辨识为主。

因为我不打算用，所以不在这里写综述。
